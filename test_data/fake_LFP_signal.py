#!/usr/bin/env python

'''
Generation of LFP like signal from spike data
---------------------------------------------

Generates "LFP" like signal for demonstration with the VIOLA tool, using a
simplified biophysics based approach.

In short, the signals are generated by
    1.  Computing the extracellular potential around ball and stick neurons
        with attributes inherited 

The setup is such that (i) all spikes from the network is assigned to bins of
0.4 x 0.4 mm in a 10 x 10 layout covering the network, with a time resolution
of 1 ms. The resulting binwise rate profiles are convolved with spatiotemporal
kernels that could in principle be representative of the spike-lfp relation
when network correlations are ignored.


The main output is the file out_proc/LFPdata.lfp containing the
signal in 100 channels across the network plane.

Usage:
::

    python fake_LFP_signal.py out_raw out_proc
'''

from __future__ import division

import matplotlib
import os
if 'jr' in os.environ['HOSTNAME'] or 'blaustein' in os.environ['HOSTNAME']:
    matplotlib.use('Agg')

import sys
import numpy as np
import scipy.signal as ss
import nest_preprocessing as npr
import topo_brunel_alpha_nest as network
import LFPy
import neuron
import quantities as pq
import matplotlib.pyplot as plt

def decimate(x, q=10, n=4, k=0.8, filterfun=ss.cheby1):
    """
    scipy.signal.decimate like downsampling using filtfilt instead of lfilter,
    and filter coeffs from butterworth or chebyshev type 1.


    Parameters
    ----------
    x : numpy.ndarray
        Array to be downsampled along last axis.
    q : int 
        Downsampling factor.
    n : int
        Filter order.
    k : float
        Aliasing filter critical frequency Wn will be set as Wn=k/q.
    filterfun : function
        `scipy.signal.filter_design.cheby1` or
        `scipy.signal.filter_design.butter` function

    Returns
    -------
    numpy.ndarray
        Array of downsampled signal.
              
    """
    if not isinstance(q, int):
        raise TypeError("q must be an integer")

    if n is None:
        n = 1

    if filterfun == ss.butter:
        b, a = filterfun(n, k / q)
    elif filterfun == ss.cheby1:
        b, a = filterfun(n, 0.05, k / q)
    else:
        raise Exception('only ss.butter or ss.cheby1 supported')

    try:
        y = ss.filtfilt(b, a, x)
    except: # Multidim array can only be processed at once for scipy >= 0.9.0
        y = []
        for data in x:
            y.append(ss.filtfilt(b, a, data))
        y = np.array(y)

    try:
        return y[:, ::q]
    except:
        return y[::q]

def compute_h(L_dend, r_dend, r_soma, cellParams, synParams, electrodeParams, section='soma'):
    # create ball soma and stick dendrite model
    soma = neuron.h.Section(name='soma')
    soma.diam = r_soma*2
    soma.L = r_soma*2
    # NOTE: Consider setting soma diameter and length by preserving
    # input impedance from network point neurons for a fixed width and length
    # dendrite section.
    
    dend = neuron.h.Section(name='dend')
    dend.diam = r_dend*2.
    dend.L = L_dend
    
    # connect
    dend.connect(soma(1.), 0.)
    
    # define SectionList
    morphology=neuron.h.SectionList()
    morphology.append(sec=soma)
    morphology.append(sec=dend)
    
    # instantiate LFPy.Cell class
    cell = LFPy.Cell(morphology=morphology, **cellParams)
    cell.set_pos(0, 0, 0)
    cell.set_rotation(y=-np.pi/2)
        
    # instantiate LFPy.Synapse class
    try:
        assert(hasattr(neuron.h, 'AlphaISyn'))
    except AttributeError:
        raise AttributeError('no AlphaISyn mech found, run nrnivmodl inside this folder')
    
    # create synapses, distribute across entire section so we divide
    # the total synapse input by number of segments in section
    idx = cell.get_idx(section=section)
    weight = synParams.pop('weight')
    for i in idx:
        syn = LFPy.Synapse(cell, idx=i, weight=weight / idx.size,
                           **synParams)
        syn.set_spike_times(np.array([lag]))
    
    # run simulation of extracellular potentials
    cell.simulate(rec_imem=True)
    
    # instantiate RexExtElectrode class and compute the electrode signals    
    electrode = LFPy.RecExtElectrode(cell=cell, **electrodeParams)
    electrode.calc_lfp()
    
    
    if test_plots:
        from  matplotlib.animation import FuncAnimation
        
        electrode_xz = LFPy.RecExtElectrode(cell=cell, **electrodeParams_xz)
        electrode_xz.calc_lfp()
        
        absmax = abs(electrode_xz.LFP).max() / 2.
        
        fig, ax = plt.subplots(1,1)
        fig.suptitle(section)
        im = ax.imshow(electrode_xz.LFP[:, 250].reshape(X_xz.shape), cmap='PRGn', vmin=-absmax, vmax=absmax, interpolation='nearest', origin='lower',
                  extent=[electrode_xz.x.min(), electrode_xz.x.max(), electrode_xz.z.min(), electrode_xz.z.max()])
        plt.colorbar(im)
        def animate(i):
            im.set_data(electrode_xz.LFP[:, 250+i].reshape(X_xz.shape))
            return im,
        
        anim = FuncAnimation(fig, animate,
                             frames=100, interval=20)
        plt.show()
        electrode_xz.cell = None
        
    # clean up namespace, delete all section references
    electrode.cell = None
    syn = None
    cell = None
    morphology = None
    dend = None
    soma = None
    
    return electrode.LFP

print('perform spatiotemporal binning of network activity for LFPs')

# input and output path can be provided on the command line:
#    python fake_LFP_signal.py out_raw out_proc
# if no argument is given, default values are used
if len(sys.argv) != 3:
    input_path = 'out_raw'
    output_path = 'out_proc'
else:
    input_path = sys.argv[-2]
    output_path = sys.argv[-1]

#bin spike data on a grid with a spatial bin size .4 mm and temporal bin size
#of 1 ms
preprocess = npr.ViolaPreprocessing(input_path=input_path,
                            output_path=output_path,
                            X = ['EX', 'IN', 'STIM'],
                            t_sim = network.simtime,
                            dt = network.dt,
                            extent_length = network.extent_length,
                            GID_filename = 'population_GIDs.dat',
                            position_filename_label = 'neuron_positions-',
                            spike_detector_label = 'spikes-',
                            TRANSIENT=network.transient,
                            BINSIZE_TIME=network.dt,
                            BINSIZE_AREA=0.4,
)


#some needed attributes
preprocess.GIDs_corrected = preprocess.get_GIDs()
positions_corrected = preprocess.get_positions()
#bin-edge-coordinates in the spatially resolved spike histograms
y, x = np.meshgrid(preprocess.pos_bins, preprocess.pos_bins)

# compute for a sentral bin the unique distances to neighbouring and bin
# counts at similar distances.
r, r_index, r_inverse, r_counts = np.unique(np.round(np.sqrt(x**2 + y**2),
                                                     decimals=10),
                                            return_index=True,
                                            return_inverse=True,
                                            return_counts=True)
inds = r < np.sqrt(2*(preprocess.extent_length/2.)**2)
r = r[inds]
r_counts = r_counts[inds]


# temporal lag vector for the convolution. Temporal bin size of 1 ms is used.
lag = 25

print('precomputing LFP kernels using a ball and stick model')

# derive biophysics params from point-neuron network params (CMem, tauMem)
Ra = 150 # ohm-cm
cm = 1. * pq.uF / pq.cm**2
CMem = network.CMem * pq.pF
# define dendritic stick length and radius
L_dend = 500*pq.um
r_dend = 2.5*pq.um
r_soma = (CMem/(4*np.pi*cm) - L_dend*r_dend/2)**0.5
#r_soma = ((CMem / (4*np.pi*cm))**0.5) # m
# compute g_pas such that membrane time constant of point neuron preserved
tauMem = network.tauMem * pq.ms
g_pas = (cm / tauMem) # s**3*A**2/(kg*m**4) == S / m**2

# convert to dimensionless (NEURON units)
r_soma = (r_soma / pq.um).simplified
g_pas = (g_pas / (pq.S / pq.cm**2)).simplified

cellParams = dict(
    Ra=Ra,
    cm=cm,
    v_init=network.neuron_params['E_L'],
    passive=True,
    passive_parameters = dict(g_pas=g_pas,
                              e_pas=network.neuron_params['E_L'],
    ),
    delete_sections=False,
    tstop=lag*2,
    dt=network.dt    
)

synParams = dict(
    syntype = 'AlphaISyn',
    tau = network.tauSyn,
)
synParams_ex = dict(weight=network.J_ex*1E-3, **synParams)
synParams_in = dict(weight=network.J_in*1E-3, **synParams)

electrodeParams = dict(
    x = r*1E3, # mm -> um
    y = np.zeros(r.size),
    z = np.zeros(r.size),
    sigma = 0.3,
    method='soma_as_point',
    # report the averaged LFP within a square with side length 400 um
    contact_shape='square',
    N = [[0, 0, 1]]*r.size,
    r = preprocess.BINSIZE_AREA*1E3,
    n = 1000,
)

# additional set of electrode parameters, looking at potentials in the xz-pane
x_xz = np.linspace(0, 200, 21)
z_xz = np.linspace(-100, 600, 71)
X_xz, Z_xz = np.meshgrid(x_xz, z_xz)
electrodeParams_xz = dict(
    x = X_xz.flatten(),
    y = np.zeros(X_xz.size),
    z = Z_xz.flatten(),
    sigma = 0.3,
    method='soma_as_point',
)


# switch for rendering test plots
test_plots = False#True


    


# compute extracellular potentials across space for excitatory and inhibitory connections to the ball and stick,
# using reconstruction array from np.unique to create a signal across 2D space
H0 = dict()
H0['EX'] = compute_h(L_dend, r_dend, r_soma, cellParams, dict(**synParams_ex), electrodeParams, section='dend')[r_inverse].reshape(x.shape+(-1,))
H0['IN'] = compute_h(L_dend, r_dend, r_soma, cellParams, dict(**synParams_in), electrodeParams, section='soma')[r_inverse].reshape(x.shape+(-1,))
H0['STIM'] = H0['EX']

# average out-degrees computed from the fixed indegrees
outdegree_ex = network.N_neurons * network.epsilon
outdegree_in = network.N_neurons * network.epsilon
outdegree_stim = network.num_stim_conn

# construct spatial convolution kernels for each unique distance r
Hs = np.zeros(x.shape+(r.size,))
for i, d in enumerate(r):
    Hs[:, :, i] = (r[r_inverse].reshape(x.shape) == d).astype(float)
    # Don't normalize such that sum over all entries is 1.
    # (we scale signals up by indegree per bin)
    # Hs[:, :, i] /= Hs[:, :, i].sum()
    

# modify convolution kernels to account for distance-dependent connection
# probabilities and delays (as well as mean outdegree)
H = dict() # container.
for i, (X, outdegree) in enumerate(zip(preprocess.X, [outdegree_ex, outdegree_in, outdegree_stim])):
    if X == 'EX':
        sigma = network.sigma_ex
        mask_radius_stim = None
    elif X == 'IN':
        sigma = network.sigma_in
        mask_radius_stim = None
    elif X == 'STIM':
        mask_radius_stim = network.mask_radius_stim
        sigma = None

    h0 = H0[X]
    H[X] = np.zeros((r.size, h0.shape[-1]))
    
    for l, (d, count) in enumerate(zip(r, r_counts)):
        if X == 'STIM':
            delay = int(network.conn_dict_stim['delays'] / network.dt)
        elif X == 'EX':
            c = network.conn_dict_ex['delays']['linear']['c'] # ms
            a = network.conn_dict_ex['delays']['linear']['a'] # mm/ms
            delay = int((c + a*d) / network.dt) # unitless
        elif X == 'IN':
            c = network.conn_dict_in['delays']['linear']['c'] # ms
            a = network.conn_dict_in['delays']['linear']['a'] # mm/ms
            delay = int((c + a*d) / network.dt) # unitless
        else:
            raise Exception
        h_delay = np.zeros(h0.shape[-1])
        h_delay[int(lag / network.dt) + delay] = 1.
        
        # compute indegree per spatial bin at distance d as function of outdegree
        if X == 'EX':
            indegree = np.exp(-d**2 / sigma**2)*count / (np.exp(-r**2 / sigma**2)*r_counts).sum() * outdegree_ex
        elif X == 'IN':
            indegree = np.exp(-d**2 / sigma**2)*count / (np.exp(-r**2 / sigma**2)*r_counts).sum() * outdegree_in
        elif X == 'STIM':
            indegree = float(d < mask_radius_stim) / (r_counts*(r < mask_radius_stim)).sum() * outdegree_stim

        
        # translate h0 in time domain to accound for delay at this radius
        h_delayed = np.zeros(H[X].shape)
        for j, x in enumerate(h0.reshape((-1, H[X].shape[-1]))[r_index]):
            h_delayed[j, ] = np.convolve(x, h_delay, 'same')
        
        # reshape into shape of h0 for spatial convolution:
        h_delayed = h_delayed[r_inverse].reshape(h0.shape)        
        
        # spatial convolution with periodic boundaries, reshaped
        for t in range(h0.shape[-1]):            
            H[X][:, t] += ss.convolve2d(h_delayed[:, :, t], Hs[:, :, l], mode='same', boundary='symm').flatten()[r_index]*indegree

    if test_plots:
        plt.figure()
        plt.subplot(211)
        im = plt.imshow(h0.reshape((-1, h0.shape[-1]))[r_index], interpolation='nearest')
        plt.title(X)
        plt.axis('tight')
        plt.colorbar(im)
        plt.subplot(212)
        im=plt.imshow(H[X], interpolation='nearest')
        plt.axis('tight')
        plt.colorbar(im)


# bin senterpoints (flattened)
y, x = np.meshgrid(preprocess.pos_bins[:-1], preprocess.pos_bins[:-1])
x = x.flatten() + .2
y = y.flatten() + .2



# Container for reconstructed LFP per postsynaptic population
LFP_h = {}
# Iterate over presynaptic populations, then postsynaptic populations.
# Provide the corresponding kernels and scalings by outdegree
for i, X in enumerate(preprocess.X):
    print('presynaptic population {}'.format(X))
    # compute spike train histograms for each 0.4x0.4 mm bin centered on each
    # contact using same procedure as in dataset_analysis.py
    spikes = npr.read_gdf(os.path.join(preprocess.output_path,
                                        preprocess.spike_detector_label +
                                        X + '.gdf'))
    sptrains = preprocess.compute_time_binned_sptrains(X, spikes,
                                                       preprocess.time_bins_rs,
                                                       dtype=np.uint8)
    binned_sptrains = preprocess.compute_pos_binned_sptrains(positions_corrected[X],
                                                           sptrains,
                                                           dtype=np.uint16).toarray()

    # for j, Y in enumerate(preprocess.X[:-1]):
    # Set up container for LFP signal of each postsynaptic population
    # due to presynaptic activity
    if X not in LFP_h.keys():
        LFP_h[X] = np.zeros(binned_sptrains.shape)
        
    # np.convolve can only deal with 1D sequences, so we have to recursively
    # iterate over all local and non-local rate bins.
    for k in range(x.size):
        #iterate over distances.
        for l, d in enumerate(r):
            # compute rate-bin distance to other bins, taking into account
            # periodic boundary conditions
            if True: # We're using periodic boundary conditions
                xdist = np.abs(x-x[k])
                ydist = np.abs(y-y[k])
                xdist[xdist > preprocess.extent_length] = preprocess.extent_length - xdist[xdist > preprocess.extent_length]
                ydist[ydist > preprocess.extent_length] = preprocess.extent_length - ydist[ydist > preprocess.extent_length]
                R = np.round(np.sqrt(xdist**2 + ydist**2), decimals=10)
            else:
                R = np.round((np.sqrt((x-x[k])**2 + (y-y[k])**2)), decimals=10)

            inds = R == d
            if inds.sum() > 0:
                # Convolve, add contribution to compute LFP signal
                LFP_h[X][inds, ] += np.convolve(binned_sptrains[k, ], H[X][l, ], 'same')

# downsample LFP signals to a time resolution of 1 ms. 
for key, value in LFP_h.items():
    LFP_h[key] = decimate(value, q=int(1/network.dt))

# write the cell-type specific output files to hdf5.
# Compute the compound signal as well
if False:
    LFP_approx = np.zeros_like(LFP_h[X])
    for key, value in LFP_h.items():
        LFP_approx += value
        f = h5py.File(os.path.join(preprocess.output_path, key + 'LFP.h5'), 'w')
        f['data'] = value
        f['srate'] = 1., # / preprocess.BINSIZE_TIME
        print 'wrote file {}'.format(f)
        f.close()

    # write compound output file
    f = h5py.File(os.path.join(preprocess.output_path, 'LFP.h5'), 'w')
    f['data'] = LFP_approx
    f['srate'] = 1., # / preprocess.BINSIZE_TIME
    print 'wrote file {}'.format(f)
    f.close()

    #dump data as text file for use with the Visualizer.
    os.system('h5dump -d data -o {}/LFPData.lfp -w 0 -y {}/LFP.h5'.format(
        preprocess.output_path, preprocess.output_path))
else:
    # save directly on text-based file format the visualizer can read,
    # as hdf5 is not supported. First the population specific signals, that
    # will be identical by design, then, the compound signal
    LFP_approx = np.zeros_like(LFP_h[X])
    for key, value in LFP_h.items():
        # subtract temporal mean in each channel
        value = (value.T - value.mean(axis=1)).T
        LFP_approx += value
        np.savetxt(os.path.join(preprocess.output_path, key + 'LFPdata.lfp'),
                   value, fmt='%.4f', delimiter=', ')

    # compound signal
    np.savetxt(os.path.join(preprocess.output_path, 'LFPdata.lfp'), LFP_approx,
               fmt='%.4f', delimiter=', ')


if test_plots:
    for key, value in LFP_h.items():
        value = (value.T - value.mean(axis=1)).T
        plt.figure()
        plt.imshow(value, interpolation='nearest'); plt.axis('tight'); plt.axis('tight'); plt.colorbar()
        plt.title(key)
    plt.show()




    from  matplotlib.animation import FuncAnimation
    
    fig, ax = plt.subplots(1,1)
    fig.suptitle('LFP, t=0')
    shape = (preprocess.pos_bins.size -1, preprocess.pos_bins.size -1)
    absmax = abs(LFP_approx).max() / 5
    im = ax.imshow(LFP_approx[:, 0].reshape(shape), cmap='PRGn', vmin=-absmax, vmax=absmax, interpolation='nearest', origin='lower',
              extent=[preprocess.pos_bins.min(), preprocess.pos_bins.max(), preprocess.pos_bins.min(), preprocess.pos_bins.max()])
    plt.colorbar(im)
    def animate(i):
        fig.suptitle('LFP, t={}'.format(i))
        im.set_data(LFP_approx[:, i].reshape(shape))
        return im,
    
    anim = FuncAnimation(fig, animate,
                         frames=LFP_approx.shape[1], interval=20)
    
    plt.show()
        


print('done!')
